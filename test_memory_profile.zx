# Memory profiling test
print("=== Memory Profiling ===\n")

# Test 1: Empty list baseline
let baseline = memory_stats().current
print("Baseline memory: " + string(baseline) + " bytes")

# Test 2: Simple integers
let simple_list = []
let i = 0
while i < 1000 {
    simple_list = simple_list + [i]
    i = i + 1
}
let after_simple = memory_stats().current
print("After 1000 integers: " + string(after_simple - baseline) + " bytes (" + string((after_simple - baseline) / 1000) + " bytes/item)")

# Test 3: Complex nested structures (like test 1.3)
let complex_list = []
let j = 0
while j < 1000 {
    complex_list = complex_list + [{
        id: j,
        name: "Item " + string(j),
        data: "x" * 100,
        nested: {a: 1, b: 2, c: [1, 2, 3]}
    }]
    j = j + 1
}
let after_complex = memory_stats().current
print("After 1000 complex items: " + string(after_complex - after_simple) + " bytes (" + string((after_complex - after_simple) / 1000) + " bytes/item)")

# Test 4: Check list reallocation overhead
let test_list = []
let k = 0
let mem_before = memory_stats().current
while k < 100 {
    test_list = test_list + [k]  # This creates a new list every time!
    k = k + 1
}
let mem_after = memory_stats().current
print("\nList concatenation overhead test:")
print("100 items via concatenation: " + string(mem_after - mem_before) + " bytes")
print("This suggests list copying overhead!")

print("\n=== Analysis ===")
print("The high memory per item (~7.4 KB) is likely due to:")
print("1. List concatenation creates a new list each iteration")
print("2. Old lists aren't immediately freed (GC delay)")
print("3. Python object overhead for each Zexus object")
print("4. String interning and representation overhead")
